{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "import nltk\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('C:\\\\Users\\\\Insti\\\\Desktop\\\\input\\\\health_tweets.csv') #.../input/health_tweets.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bbchealth</td>\n",
       "      <td>How the UK’s coronavirus epidemic compares to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bbchealth</td>\n",
       "      <td>Health workers on frontline to be tested in En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bbchealth</td>\n",
       "      <td>Coronavirus: Protective gear guidance 'to be u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bbchealth</td>\n",
       "      <td>Coronavirus: What are ventilators and why are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bbchealth</td>\n",
       "      <td>Coronavirus: 'Act early to save more than 30 m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    username                                              tweet\n",
       "0  bbchealth  How the UK’s coronavirus epidemic compares to ...\n",
       "1  bbchealth  Health workers on frontline to be tested in En...\n",
       "2  bbchealth  Coronavirus: Protective gear guidance 'to be u...\n",
       "3  bbchealth  Coronavirus: What are ventilators and why are ...\n",
       "4  bbchealth  Coronavirus: 'Act early to save more than 30 m..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df of handle and tweet only to easily see how tweet is formatted\n",
    "simple_df  = tweets_df[['username', 'tweet']]\n",
    "simple_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = simple_df.sample(n=10, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_8306529e_736f_11ea_b671_c8348e237130row0_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row1_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row2_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row3_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row4_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row5_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row6_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row7_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row8_col1 {\n",
       "            width:  500px;\n",
       "        }    #T_8306529e_736f_11ea_b671_c8348e237130row9_col1 {\n",
       "            width:  500px;\n",
       "        }</style><table id=\"T_8306529e_736f_11ea_b671_c8348e237130\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >username</th>        <th class=\"col_heading level0 col1\" >tweet</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row0\" class=\"row_heading level0 row0\" >8336</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row0_col0\" class=\"data row0 col0\" >bbchealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row0_col1\" class=\"data row0 col1\" >NI test results due over Ebola virus  http://bbc.in/10Pccbm </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row1\" class=\"row_heading level0 row1\" >65222</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row1_col0\" class=\"data row1 col0\" >HarvardHealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row1_col1\" class=\"data row1 col1\" >Is the food pyramid still accurate? No, but the healthy eating plate is:  http://hvrd.me/Kx1Lv   pic.twitter.com/Bw8k9JGL89 via @HarvardAskDrK</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row2\" class=\"row_heading level0 row2\" >117965</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row2_col0\" class=\"data row2 col0\" >NYTHealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row2_col1\" class=\"data row2 col1\" >Early pregnancy loss is not uncommon. Many women miscarry before they know they’re pregnant. https://nyti.ms/2HFUOwZ </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row3\" class=\"row_heading level0 row3\" >78508</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row3_col0\" class=\"data row3 col0\" >KHNews</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row3_col1\" class=\"data row3 col1\" >Emergency Rooms Are Front Line For Enrolling New Obamacare Customers, @SarahVarney reports:  http://khne.ws/1d0ITnX </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row4\" class=\"row_heading level0 row4\" >2347</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row4_col0\" class=\"data row4 col0\" >bbchealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row4_col1\" class=\"data row4 col1\" >Organ donation campaigner, 18, in New Year Honours http://bbc.in/2DyFCyh </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row5\" class=\"row_heading level0 row5\" >86113</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row5_col0\" class=\"data row5 col0\" >NBCNewsHealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row5_col1\" class=\"data row5 col1\" >Juul to ban most of its flavored products from retail stores  https://nbcnews.to/2DkcDBa </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row6\" class=\"row_heading level0 row6\" >60718</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row6_col0\" class=\"data row6 col0\" >HarvardHealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row6_col1\" class=\"data row6 col1\" >The more meat you eat, the higher your risk of diabetes, heart disease, and stroke is. #HarvardHealth #nutrition  http://bit.ly/2Roxac2  pic.twitter.com/Rs6ovTnV9J</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row7\" class=\"row_heading level0 row7\" >28377</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row7_col0\" class=\"data row7 col0\" >foxnewshealth</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row7_col1\" class=\"data row7 col1\" >The report calls for screening of all children by the time the turn 1 and again between ages 2 and 3  http://fxn.ws/2fksilh </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row8\" class=\"row_heading level0 row8\" >164951</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row8_col0\" class=\"data row8 col0\" >Reuters_Health</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row8_col1\" class=\"data row8 col1\" >Halozyme shares jump after FDA lifts hold on trial  http://reut.rs/1nl9RM6 </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8306529e_736f_11ea_b671_c8348e237130level0_row9\" class=\"row_heading level0 row9\" >147470</th>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row9_col0\" class=\"data row9 col0\" >Reuters_Health</td>\n",
       "                        <td id=\"T_8306529e_736f_11ea_b671_c8348e237130row9_col1\" class=\"data row9 col1\" >Elite athletes not at higher risk of birth complications  https://reut.rs/2CSLdmJ </td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x214cc1841c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.style.set_properties(subset=['tweet'], **{'width': '500px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retween and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stemming for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Insti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'         # define a string of punctuation symbols\n",
    "\n",
    "# main function to clean tweet\n",
    "def clean_tweet(tweet, bigrams=False):  # master function to clean tweet\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet_token_list = [word for word in tweet.split(' ')\n",
    "                            if word not in stopwords] # remove stopwords\n",
    "#     tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
    "#                         for word in tweet_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clean_df_stem = simple_df.tweet.apply(clean_tweet).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets after cleaning (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clean_stem = simple_clean_df_stem.sample(n=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row0_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row1_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row2_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row3_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row4_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row5_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row6_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row7_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row8_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_ac484cf0_736f_11ea_b1ab_c8348e237130row9_col0 {\n",
       "            width:  500px;\n",
       "        }</style><table id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >tweet</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row0\" class=\"row_heading level0 row0\" >8336</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row0_col0\" class=\"data row0 col0\" >ni test results due ebola virus </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row1\" class=\"row_heading level0 row1\" >65222</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row1_col0\" class=\"data row1 col0\" >food pyramid still accurate healthy eating plate via </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row2\" class=\"row_heading level0 row2\" >117965</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row2_col0\" class=\"data row2 col0\" >early pregnancy loss uncommon many women miscarry know they’re pregnant </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row3\" class=\"row_heading level0 row3\" >78508</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row3_col0\" class=\"data row3 col0\" >emergency rooms front line enrolling new obamacare customers reports </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row4\" class=\"row_heading level0 row4\" >2347</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row4_col0\" class=\"data row4 col0\" >organ donation campaigner  new year honours </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row5\" class=\"row_heading level0 row5\" >86113</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row5_col0\" class=\"data row5 col0\" >juul ban flavored products retail stores </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row6\" class=\"row_heading level0 row6\" >60718</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row6_col0\" class=\"data row6 col0\" >meat eat higher risk diabetes heart disease stroke #harvardhealth #nutrition </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row7\" class=\"row_heading level0 row7\" >28377</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row7_col0\" class=\"data row7 col0\" >report calls screening children time turn  ages   </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row8\" class=\"row_heading level0 row8\" >164951</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row8_col0\" class=\"data row8 col0\" >halozyme shares jump fda lifts hold trial </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130level0_row9\" class=\"row_heading level0 row9\" >147470</th>\n",
       "                        <td id=\"T_ac484cf0_736f_11ea_b1ab_c8348e237130row9_col0\" class=\"data row9 col0\" >elite athletes higher risk birth complications </td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x214ce6a86c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_clean_stem.style.set_properties(subset=['tweet'], **{'width': '500px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lemmatization for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Insti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'         # define a string of punctuation symbols\n",
    "\n",
    "## functions leveraged from https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "\n",
    "def lemmatize_stemming(tweet):\n",
    "    return WordNetLemmatizer().lemmatize(tweet, pos='v')\n",
    "\n",
    "# tokenize and lemmatize\n",
    "def lemmatize(tweet):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:   # drops words with 3 or less characters\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def clean_tweet(tweet, bigrams=False):   # master function to clean tweet\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    \n",
    "    tweet_token_list = lemmatize(tweet)  # apply lemmatization and tokenization\n",
    "\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clean_df_lemma = simple_df.tweet.apply(clean_tweet).to_frame()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets after cleaning (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clean_lemma = simple_clean_df_lemma.sample(n=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_5850ddca_7371_11ea_9273_c8348e237130row0_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row1_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row2_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row3_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row4_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row5_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row6_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row7_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row8_col0 {\n",
       "            width:  500px;\n",
       "        }    #T_5850ddca_7371_11ea_9273_c8348e237130row9_col0 {\n",
       "            width:  500px;\n",
       "        }</style><table id=\"T_5850ddca_7371_11ea_9273_c8348e237130\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >tweet</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row0\" class=\"row_heading level0 row0\" >8336</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row0_col0\" class=\"data row0 col0\" >test result ebola virus</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row1\" class=\"row_heading level0 row1\" >65222</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row1_col0\" class=\"data row1 col0\" >food pyramid accurate healthy eat plate</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row2\" class=\"row_heading level0 row2\" >117965</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row2_col0\" class=\"data row2 col0\" >early pregnancy loss uncommon women miscarry know pregnant</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row3\" class=\"row_heading level0 row3\" >78508</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row3_col0\" class=\"data row3 col0\" >emergency room line enrol obamacare customers report</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row4\" class=\"row_heading level0 row4\" >2347</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row4_col0\" class=\"data row4 col0\" >organ donation campaigner year honour</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row5\" class=\"row_heading level0 row5\" >86113</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row5_col0\" class=\"data row5 col0\" >juul flavor products retail store</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row6\" class=\"row_heading level0 row6\" >60718</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row6_col0\" class=\"data row6 col0\" >meat higher risk diabetes heart disease stroke harvardhealth nutrition</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row7\" class=\"row_heading level0 row7\" >28377</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row7_col0\" class=\"data row7 col0\" >report call screen children time turn age</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row8\" class=\"row_heading level0 row8\" >164951</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row8_col0\" class=\"data row8 col0\" >halozyme share jump lift hold trial</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5850ddca_7371_11ea_9273_c8348e237130level0_row9\" class=\"row_heading level0 row9\" >147470</th>\n",
       "                        <td id=\"T_5850ddca_7371_11ea_9273_c8348e237130row9_col0\" class=\"data row9 col0\" >elite athletes higher risk birth complications</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x214ce21dfc8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_clean_lemma.style.set_properties(subset=['tweet'], **{'width': '500px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## alternate approach to cleaning tweet data\n",
    "# ## taken from https://towardsdatascience.com/topic-modeling-of-2019-hr-tech-conference-twitter-d16cf75895b6\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# import nltk\n",
    "\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"\n",
    "#     Map POS tag to first character lemmatize() accepts\n",
    "#     \"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# text_clean = text.lower()\n",
    "# # Remove non-alphabet\n",
    "# text_clean = re.sub(r'[^a-zA-Z]|(\\w+:\\/\\/\\S+)',' ', text_clean).split()    \n",
    "# # Remove short words (length < 3)\n",
    "# text_clean = [w for w in text_clean if len(w)>2]\n",
    "# # Lemmatize text with the appropriate POS tag\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# text_clean = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text_clean]\n",
    "# # Filter out stop words in English \n",
    "# stops = set(stopwords.words('english')).union(additional_stop_words)\n",
    "# text_clean = [w for w in text_clean if w not in stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for counts of Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(tweet, bigrams=True):   # master function to clean tweet\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    \n",
    "    tweet_token_list = lemmatize(tweet)  # apply lemmatization and tokenization\n",
    "\n",
    "    if bigrams:\n",
    "        tweet_token_list = [tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    #tweet_token_list + (taken from above assignment)\n",
    "    tweet_token_list= ' '.join(tweet_token_list)\n",
    "    return tweet_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = simple_df.tweet.apply(find_bigrams).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus_epidemic epidemic_compare compare_countries\n"
     ]
    }
   ],
   "source": [
    "#check bigram example\n",
    "print((bigrams.iloc[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=500, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(bigrams['tweet']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "bigram_names = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blood_pressure       664.0\n",
       "breast_cancer       1308.0\n",
       "cancer_drug          720.0\n",
       "drug_price           609.0\n",
       "health_care         2761.0\n",
       "health_insurance     752.0\n",
       "health_officials     544.0\n",
       "heart_attack         680.0\n",
       "heart_disease        738.0\n",
       "long_term            522.0\n",
       "mental_health       1759.0\n",
       "need_know            756.0\n",
       "nurse_home           526.0\n",
       "public_health        667.0\n",
       "study_find          1471.0\n",
       "study_say           1349.0\n",
       "study_suggest        552.0\n",
       "unite_state          525.0\n",
       "weight_loss          711.0\n",
       "zika_virus           598.0\n",
       "Name: total, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert tf to a dataframe, assign columns to bigram names, and calculate total\n",
    "tf = pd.DataFrame(tf)\n",
    "tf.columns = [bigram_names]\n",
    "sample_tf = tf.head()\n",
    "tf.loc['total',:] = tf.sum(axis=0)\n",
    "tf.loc['total']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
